{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04 Tensorflow Dataset API.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC6w1eswzg__"
      },
      "source": [
        "## Tensorflow Dataset API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5fld-VVzne7"
      },
      "source": [
        "**Learning Objectives**\r\n",
        "\r\n",
        "- Learn how to use tf.data to read data from memory\r\n",
        "- Learn how to use tf.data in a training loop\r\n",
        "- Learn how to use tf.data to read data from disk\r\n",
        "- Learn how to write production input pipelines with feature engineering (batching, shuffling, etc.)\r\n",
        "\r\n",
        "\r\n",
        "In this notebook, we will start by refactoring the linear regression we implemented in the previous lab so that it takes data from a tf.data.Dataset, and we will learn how to implement **stochastic gradient descent** with it. In this case, the original dataset will be synthetic and read by the tf.data API directly from memory.\r\n",
        "\r\n",
        "In a second part, we will learn how to load a dataset with the tf.data API when the dataset resides on *disk*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH_koIxCxa6h"
      },
      "source": [
        "# The json modeule is used to convert python dictionary into a json file that is written in the file\r\n",
        "import json\r\n",
        "import math # for arithmetic operations\r\n",
        "import os # for interacting with Operating System\r\n",
        "from pprint import pprint # The pprint module provides a capability to `pretty-print` arbitrary Python data structures in a form which can be used as input to the interpreter\r\n",
        "\r\n",
        "import numpy as np \r\n",
        "import tensorflow as tf "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwDJZXm50nCT"
      },
      "source": [
        "## Loading data from Memory \r\n",
        "\r\n",
        "### Creating the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdmH76As0jr8",
        "outputId": "ce772dc0-daf1-4437-f64f-956ad7e70a12"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtFK_KEb1q2D"
      },
      "source": [
        "X = tf.constant(range(10), dtype=tf.float32)\r\n",
        "Y = 2 * X + 10"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR3RJZyI2Izh"
      },
      "source": [
        "\r\n",
        "We begin with implementing a function that takes as input\r\n",
        "\r\n",
        "- our $X$ and $Y$ vectors of synthetic data generated by the linear function $y= 2x + 10$\r\n",
        "\r\n",
        "- the number of passes over the dataset we want to train on (epochs)\r\n",
        "\r\n",
        "- the size of the batches the dataset (batch_size)\r\n",
        "\r\n",
        "- and returns a tf.data.Dataset:\r\n",
        "\r\n",
        "**Remark:** Note that the last batch may not contain the exact number of elements you specified because the dataset was exhausted.\r\n",
        "\r\n",
        "If you want batches with the exact same number of elements per batch, we will have to discard the last batch by setting:\r\n",
        "\r\n",
        "dataset = dataset.batch(batch_size, drop_remainder=True)\r\n",
        "We will do that here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfkiSl3w1287"
      },
      "source": [
        "def create_dataset(X,Y,epochs,batch_size):\r\n",
        "  # Using the tf.data.Dataset.from_tensor_slices() method we are able to get the slices of list or array\r\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((X,Y))\r\n",
        "  dataset = dataset.repeat(epochs).batch(batch_size,drop_remainder=True)\r\n",
        "  return dataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm_rSVuG3TAC"
      },
      "source": [
        "Let's test our function by iterating twice over our dataset in batches of 3 datapoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNpuck6y3Pp_",
        "outputId": "154d0d6f-8e04-4bad-9a1c-b980ef6f5ab9"
      },
      "source": [
        "BATCH_SIZE = 3\r\n",
        "EPOCHS = 2\r\n",
        "\r\n",
        "dataset = create_dataset(X,Y,epochs=1, batch_size=3)\r\n",
        "\r\n",
        "for i, (x,y) in enumerate(dataset):\r\n",
        "  print(\"X:\",x.numpy(),\"y:\",y.numpy())\r\n",
        "\r\n",
        "  assert len(x) == BATCH_SIZE\r\n",
        "  assert len(y) == BATCH_SIZE\r\n",
        "\r\n",
        "assert EPOCHS"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X: [0. 1. 2.] y: [10. 12. 14.]\n",
            "X: [3. 4. 5.] y: [16. 18. 20.]\n",
            "X: [6. 7. 8.] y: [22. 24. 26.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lrhVr5T4McD"
      },
      "source": [
        "## Loss Function and Gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIfTI-574B3G"
      },
      "source": [
        "# Let's define loss_mse() procedure which will return computed mean of elements across dimensions of a tensor.\r\n",
        "def loss_mse(X, Y, w0, w1):\r\n",
        "    Y_hat = w0 * X + w1\r\n",
        "    errors = (Y_hat - Y)**2\r\n",
        "    return tf.reduce_mean(errors)\r\n",
        "\r\n",
        "# Let's define compute_gradients() procedure which will return value of recorded operations for automatic differentiation\r\n",
        "def compute_gradients(X, Y, w0, w1):\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        loss = loss_mse(X, Y, w0, w1)\r\n",
        "    return tape.gradient(loss, [w0, w1])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KanM21KC4rpC"
      },
      "source": [
        "**Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGLVw42S4lt0",
        "outputId": "f591c202-760c-4a16-dd63-e6bd16a59c6e"
      },
      "source": [
        "EPOCHS = 250\r\n",
        "BATCH_SIZE = 2\r\n",
        "LEARNING_RATE = 0.02\r\n",
        "\r\n",
        "MSG = \"STEP {step} - loss: {loss}, w0: {w0}, w1: {w1}\\n\"\r\n",
        "\r\n",
        "w0 = tf.Variable(0.0)\r\n",
        "w1 = tf.Variable(0.0)\r\n",
        "\r\n",
        "dataset = create_dataset(X,Y,epochs=EPOCHS,batch_size=BATCH_SIZE)\r\n",
        "\r\n",
        "for step,(X_batch , Y_batch) in enumerate(dataset):\r\n",
        "  dw0,dw1 = compute_gradients(X_batch , Y_batch , w0 , w1)\r\n",
        "  w0.assign_sub(dw0 * LEARNING_RATE)\r\n",
        "  w1.assign_sub(dw1 * LEARNING_RATE)\r\n",
        "\r\n",
        "  if step % 100 == 0:\r\n",
        "    loss = loss_mse(X_batch , Y_batch , w0 , w1)\r\n",
        "    print(MSG.format(step = step , loss = loss , w0 = w0.numpy() , w1 = w1.numpy()))\r\n",
        "\r\n",
        "assert loss < 0.0001\r\n",
        "assert (w0 - 2) < 0.001\r\n",
        "assert (w1 - 10) < 0.001"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "STEP 0 - loss: 109.76800537109375, w0: 0.23999999463558197, w1: 0.4399999976158142\n",
            "\n",
            "STEP 100 - loss: 9.363959312438965, w0: 2.55655837059021, w1: 6.674341678619385\n",
            "\n",
            "STEP 200 - loss: 1.393267273902893, w0: 2.2146825790405273, w1: 8.717182159423828\n",
            "\n",
            "STEP 300 - loss: 0.20730558037757874, w0: 2.082810878753662, w1: 9.505172729492188\n",
            "\n",
            "STEP 400 - loss: 0.03084510937333107, w0: 2.03194260597229, w1: 9.809128761291504\n",
            "\n",
            "STEP 500 - loss: 0.004589457996189594, w0: 2.012321710586548, w1: 9.926374435424805\n",
            "\n",
            "STEP 600 - loss: 0.0006827632314525545, w0: 2.0047526359558105, w1: 9.971602439880371\n",
            "\n",
            "STEP 700 - loss: 0.00010164897685172036, w0: 2.0018346309661865, w1: 9.989042282104492\n",
            "\n",
            "STEP 800 - loss: 1.5142451957217418e-05, w0: 2.000706911087036, w1: 9.995771408081055\n",
            "\n",
            "STEP 900 - loss: 2.256260358990403e-06, w0: 2.0002737045288086, w1: 9.998367309570312\n",
            "\n",
            "STEP 1000 - loss: 3.3405058275093324e-07, w0: 2.000105381011963, w1: 9.999371528625488\n",
            "\n",
            "STEP 1100 - loss: 4.977664502803236e-08, w0: 2.000040054321289, w1: 9.999757766723633\n",
            "\n",
            "STEP 1200 - loss: 6.475602276623249e-09, w0: 2.0000154972076416, w1: 9.99991226196289\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS06gEya7EGk"
      },
      "source": [
        "## Loading Data from Disk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I2Ddb3o7HnK"
      },
      "source": [
        "**Use tf.data to read the CSV files**\r\n",
        "\r\n",
        "\r\n",
        "The tf.data API can easily read csv files using the helper function tf.data.experimental.make_csv_dataset\r\n",
        "\r\n",
        "If you have TFRecords (which is recommended), you may use tf.data.experimental.make_batched_features_dataset\r\n",
        "\r\n",
        "The first step is to define\r\n",
        "\r\n",
        "- the feature names into a list CSV_COLUMNS\r\n",
        "- their default values into a list DEFAULTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbznXx_86vd5"
      },
      "source": [
        "CSV_COLUMNS = [\r\n",
        "    'fare_amount',\r\n",
        "    'pickup_datetime',\r\n",
        "    'pickup_longitude',\r\n",
        "    'pickup_latitude',\r\n",
        "    'dropoff_longitude',\r\n",
        "    'dropoff_latitude',\r\n",
        "    'passenger_count',\r\n",
        "    'key'\r\n",
        "]\r\n",
        "\r\n",
        "LABEL_COLUMN = \"fare_amount\"\r\n",
        "\r\n",
        "DEFAULTS = [[0.0], ['na'], [0.0], [0.0], [0.0], [0.0], [0.0], ['na']]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z82_Jv_v72lN",
        "outputId": "8e6c5d0c-b601-4de1-d963-ca85569ab106"
      },
      "source": [
        "def create_dataset(pattern):\r\n",
        "  return tf.data.experimental.make_csv_dataset(pattern , 1 , CSV_COLUMNS , DEFAULTS)\r\n",
        "\r\n",
        "tempds = create_dataset(\"/content/drive/MyDrive/Datasets/Taxi/taxi-train.csv\")\r\n",
        "print(tempds)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: OrderedDict([(fare_amount, (1,)), (pickup_datetime, (1,)), (pickup_longitude, (1,)), (pickup_latitude, (1,)), (dropoff_longitude, (1,)), (dropoff_latitude, (1,)), (passenger_count, (1,)), (key, (1,))]), types: OrderedDict([(fare_amount, tf.float32), (pickup_datetime, tf.string), (pickup_longitude, tf.float32), (pickup_latitude, tf.float32), (dropoff_longitude, tf.float32), (dropoff_latitude, tf.float32), (passenger_count, tf.float32), (key, tf.string)])>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZH5Z71W56lz"
      },
      "source": [
        "Note that this is a prefetched dataset, where each element is an OrderedDict whose keys are the feature names and whose values are tensors of shape (1,) (i.e. vectors)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRcsEaI68WlQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31e430cf-d909-4fb4-ca2c-0677384d38ca"
      },
      "source": [
        "# Let's iterate over the first two element of this dataset using `dataset.take(2)`.\r\n",
        "# Then convert them ordinary Python dictionary with numpy array as values for more readability:\r\n",
        "\r\n",
        "for data in tempds.take(2):\r\n",
        "  pprint({k: v.numpy() for k , v in data.items()})\r\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'dropoff_latitude': array([40.730534], dtype=float32),\n",
            " 'dropoff_longitude': array([-73.98619], dtype=float32),\n",
            " 'fare_amount': array([8.5], dtype=float32),\n",
            " 'key': array([b'2640'], dtype=object),\n",
            " 'passenger_count': array([1.], dtype=float32),\n",
            " 'pickup_datetime': array([b'2014-12-22 19:41:41 UTC'], dtype=object),\n",
            " 'pickup_latitude': array([40.755985], dtype=float32),\n",
            " 'pickup_longitude': array([-73.97253], dtype=float32)}\n",
            "\n",
            "\n",
            "{'dropoff_latitude': array([40.677856], dtype=float32),\n",
            " 'dropoff_longitude': array([-73.98826], dtype=float32),\n",
            " 'fare_amount': array([5.5], dtype=float32),\n",
            " 'key': array([b'6448'], dtype=object),\n",
            " 'passenger_count': array([1.], dtype=float32),\n",
            " 'pickup_datetime': array([b'2014-02-24 18:22:00 UTC'], dtype=object),\n",
            " 'pickup_latitude': array([40.677395], dtype=float32),\n",
            " 'pickup_longitude': array([-73.97987], dtype=float32)}\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6RFME047ICW"
      },
      "source": [
        "## Transforming the Features\r\n",
        "\r\n",
        "\r\n",
        "What we really need is a dictionary of features + a label. So, we have to do two things to the above dictionary:\r\n",
        "\r\n",
        "- Remove the unwanted column \"key\"\r\n",
        "- Keep the label separate from the features\r\n",
        "\r\n",
        "Let's first implement a function that takes as input a row (represented as an OrderedDict in our tf.data.Dataset as above) and then returns a tuple with two elements:\r\n",
        "\r\n",
        "- The first element being the same OrderedDict with the label dropped\r\n",
        "- The second element being the label itself (fare_amount)\r\n",
        "Note that we will need to also remove the key and pickup_datetime column, which we won't use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC_dqy0H6dy3"
      },
      "source": [
        "UNWANTED_COLS = [\"pickup_datetime\", \"key\"]\r\n",
        "\r\n",
        "def features_and_labels(row_data):\r\n",
        "  labels = row_data.pop(LABEL_COLUMN)\r\n",
        "  features = row_data\r\n",
        "\r\n",
        "  for unwanter_cols in UNWANTED_COLS:\r\n",
        "    features.pop(unwanter_cols)\r\n",
        "\r\n",
        "  return features,labels"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne8FQqt-7726",
        "outputId": "76a32d66-68e3-4e9b-a721-2130073dedb9"
      },
      "source": [
        "for row_data in tempds.take(2):\r\n",
        "  features,labels = features_and_labels(row_data)\r\n",
        "  pprint(features)\r\n",
        "  print(labels, \"\\n\")\r\n",
        "\r\n",
        "  assert UNWANTED_COLS[0] not in features.keys()\r\n",
        "  assert UNWANTED_COLS[1] not in features.keys()\r\n",
        "  assert labels.shape == 1"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('pickup_longitude',\n",
            "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.972305], dtype=float32)>),\n",
            "             ('pickup_latitude',\n",
            "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.76222], dtype=float32)>),\n",
            "             ('dropoff_longitude',\n",
            "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.98782], dtype=float32)>),\n",
            "             ('dropoff_latitude',\n",
            "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.76145], dtype=float32)>),\n",
            "             ('passenger_count',\n",
            "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>)])\n",
            "tf.Tensor([8.], shape=(1,), dtype=float32) \n",
            "\n",
            "OrderedDict([('pickup_longitude',\n",
            "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.978676], dtype=float32)>),\n",
            "             ('pickup_latitude',\n",
            "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.76279], dtype=float32)>),\n",
            "             ('dropoff_longitude',\n",
            "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.977646], dtype=float32)>),\n",
            "             ('dropoff_latitude',\n",
            "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.77614], dtype=float32)>),\n",
            "             ('passenger_count',\n",
            "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>)])\n",
            "tf.Tensor([5.7], shape=(1,), dtype=float32) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBdQ4xDC9BfG"
      },
      "source": [
        "## Batching\r\n",
        "\r\n",
        "\r\n",
        "Let's now refactor our create_dataset function so that it takes an additional argument batch_size and batch the data correspondingly. We will also use the features_and_labels function we implemented for our dataset to produce tuples of features and labels.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcArBlif8iBn"
      },
      "source": [
        "def create_dataset(pattern, batch_size):\r\n",
        "  dataset =  tf.data.experimental.make_csv_dataset(pattern , batch_size , CSV_COLUMNS , DEFAULTS)\r\n",
        "\r\n",
        "  return dataset.map(features_and_labels)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMLFRHTB9mQS",
        "outputId": "dc60c464-5cf9-4fe8-8123-274e933e4a14"
      },
      "source": [
        "BATCH_SIZE = 2\r\n",
        "tempds = tempds = create_dataset(\"/content/drive/MyDrive/Datasets/Taxi/taxi-train.csv\" , batch_size= 2)\r\n",
        "\r\n",
        "for X_batch , Y_batch in tempds.take(2):\r\n",
        "  pprint({k: v.numpy() for k , v in X_batch.items()})\r\n",
        "  print(Y_batch.numpy(),\"\\n\")\r\n",
        "\r\n",
        "  assert len(Y_batch) == BATCH_SIZE"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'dropoff_latitude': array([40.76303, 40.70576], dtype=float32),\n",
            " 'dropoff_longitude': array([-73.96674 , -74.009964], dtype=float32),\n",
            " 'passenger_count': array([5., 2.], dtype=float32),\n",
            " 'pickup_latitude': array([40.719963, 40.738842], dtype=float32),\n",
            " 'pickup_longitude': array([-74.00014, -73.98321], dtype=float32)}\n",
            "[17.  13.3] \n",
            "\n",
            "{'dropoff_latitude': array([40.75059, 40.75461], dtype=float32),\n",
            " 'dropoff_longitude': array([-73.98078, -73.9798 ], dtype=float32),\n",
            " 'passenger_count': array([5., 1.], dtype=float32),\n",
            " 'pickup_latitude': array([40.740936, 40.761875], dtype=float32),\n",
            " 'pickup_longitude': array([-73.98156, -73.97502], dtype=float32)}\n",
            "[7. 4.] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBXGKJ9r-5rO"
      },
      "source": [
        "## Shuffling\r\n",
        "\r\n",
        "When training a deep learning model in batches over multiple workers, it is helpful if we shuffle the data. That way, different workers will be working on different parts of the input file at the same time, and so averaging gradients across workers will help. Also, during training, we will need to read the data indefinitely.\r\n",
        "\r\n",
        "Let's refactor our create_dataset function so that it shuffles the data, when the dataset is used for training.\r\n",
        "\r\n",
        "We will introduce an additional argument mode to our function to allow the function body to distinguish the case when it needs to shuffle the data (mode == \"train\") from when it shouldn't (mode == \"eval\").\r\n",
        "\r\n",
        "Also, before returning we will want to prefetch 1 data point ahead of time (dataset.prefetch(1)) to speed-up training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLTV2cKy-RNQ"
      },
      "source": [
        "def create_dataset(pattern, batch_size = 1, mode= \"eval\"):\r\n",
        "  dataset =  tf.data.experimental.make_csv_dataset(pattern , batch_size , CSV_COLUMNS , DEFAULTS)\r\n",
        "\r\n",
        "  dataset.map(features_and_labels)\r\n",
        "\r\n",
        "  if mode == \"train\":\r\n",
        "    dataset = dataset.shuffle(1000).repeat()\r\n",
        "\r\n",
        "    dataset = dataset.prefetch(1)\r\n",
        "\r\n",
        "    return dataset"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JZk0Ar2AFR7",
        "outputId": "865ee293-b432-4355-be71-2022eb98fc6e"
      },
      "source": [
        "tempds = create_dataset(\"/content/drive/MyDrive/Datasets/Taxi/taxi-train.csv\" , 2, \"train\")\r\n",
        "print(list(tempds.take(1)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[OrderedDict([('fare_amount', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([7.5, 9.3], dtype=float32)>), ('pickup_datetime', <tf.Tensor: shape=(2,), dtype=string, numpy=\n",
            "array([b'2014-12-08 21:50:00 UTC', b'2012-02-21 11:53:00 UTC'],\n",
            "      dtype=object)>), ('pickup_longitude', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-73.97611, -73.98924], dtype=float32)>), ('pickup_latitude', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([40.76469 , 40.739544], dtype=float32)>), ('dropoff_longitude', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-73.9792, -73.9966], dtype=float32)>), ('dropoff_latitude', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([40.75333, 40.72363], dtype=float32)>), ('passenger_count', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([6., 1.], dtype=float32)>), ('key', <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'881', b'3511'], dtype=object)>)])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5veDMMvGuLU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}